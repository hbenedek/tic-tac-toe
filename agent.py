from abc import abstractclassmethod
import numpy as np
from tic_env import TictactoeEnv, OptimalPlayer
from tqdm import tqdm
from collections import defaultdict
from utils import *
import torch
import torch.nn as nn
import torch.nn.functional as F
import random

class Agent:
    def __init__(self, player):
        self.player = player
        self.other_player = str(({'X', 'O'} - set(self.player)).pop())

    def change_player(self):
        self.player = str(({'X', 'O'} - set(self.player)).pop())

    def empty(self, grid):
        '''return all empty positions'''
        avail = []
        for i in range(9):
            pos = (int(i/3), i % 3)
            if grid[pos] == 0:
                avail.append(pos)
        return avail
 
    def randomMove(self, grid):
        """ Chose a random move from the available options. """
        avail = self.empty(grid)
        return avail[random.randint(0, len(avail)-1)]

    @abstractclassmethod
    def act(self):
        pass

class QPlayer(Agent):
    def __init__(self, epsilon=0.2, alpha=0.05, gamma=0.99, player='X', explore=False, e_min=0.1, e_max=0.8, n_star=20000):
        super().__init__(player)
        self.epsilon = epsilon
        self.alpha = alpha
        self.gamma = gamma
        self.Qvalues = defaultdict(lambda: defaultdict(int))
        self.last_action = 0
        self.last_state = 0
        self.player2value = {'X': 1, 'O': -1}
        self.counter = 0
        self.explore = explore
        self.e_min = e_min
        self.e_max = e_max
        self.n_star = n_star

    def act(self, grid):
        if self.explore:
            self.epsilon = self.decreasing_exploration(self.e_min, self.e_max, self.n_star, self.counter)
        # epsilon greedy policy w.r.t. Q-values
        if random.random() < self.epsilon:
            action = self.randomMove(grid)
            self.last_action = tuple_to_int(action)
        else:
            idx = np.argmax([self.Qvalues[str(grid)][tuple_to_int(pos)] for pos in self.empty(grid)])
            action = self.empty(grid)[idx]
            self.last_action = tuple_to_int(action)

        self.last_state = str(grid)  
        return action

    def updateQ(self, grid, reward):
        future_estimate = max([self.Qvalues[str(grid)][tuple_to_int(pos)] for pos in self.empty(grid)], default=0)
        self.Qvalues[self.last_state][self.last_action] += self.alpha * (reward + self.gamma * future_estimate - self.Qvalues[self.last_state][self.last_action]) 
            
    def decreasing_exploration(self, e_min, e_max, n_star, n):
        return max(e_min, e_max * (1 - n/n_star)) 

class DeepAgent(Agent):
    def __init__(self, epsilon=0.2, gamma=0.99, buffer=10000, batch=64, update_target=500):
        self.gamma = gamma
        self.buffer = buffer
        self.batch = batch
        self.epsilon = epsilon
        self.update_target = update_target
        self.counter = 0
        self.DQN = DeepQNetwork()
        self.target_network = DeepQNetwork()
        self.memory = Memory(self.buffer)
        self.last_state = None
        self.last_action = None
        self.last_reward = None
        self.losses = []

    def act(self, grid):
        # epsilon greedy w.r.t. Qvalues generated by Deep Network
        if random.random() < self.epsilon:
            action = self.randomMove(grid)
            self.last_action = tuple_to_int(action)
        else:
            with torch.no_grad():
                action = self.DQN(grid_to_tensor(grid)).argmax().item()
                self.last_action = action

        self.last_state = grid_to_tensor(grid)  
        return action

    def play_game(self, agent, env, i):
        grid, end, __  = env.observe()
        if i % 2 == 0:
            self.player = 'X'
            agent.player = 'O'
        else:
            self.player = 'O'
            agent.player = 'X'
        while end == False:
            if env.current_player == self.player:
                move = self.act(grid) 
                grid, end, winner = env.step(move, print_grid=False)
            else:
                move = agent.act(grid)
                grid, end, winner = env.step(move, print_grid=False) 
                if not end:
                    reward = env.reward(self.player)
                    self.memory.update(self.last_state, self.last_action, reward, grid.copy())
                
        reward = env.reward(self.player)
        self.memory.update(self.last_state, self.last_action, reward, grid.copy())
        return winner

    
    def learn(self, agent, N=20000):
        history = []
        env = TictactoeEnv()
        for i in tqdm(range(2, N+2)):
            env.reset()
            winner = self.play_game(agent, env, i) 

            if self.memory.memory_used > self.batch:
                # train phase

                # sample mini-batch from memory
                state, action, reward, next_state = self.memory.sample(self.batch)

                # calculate "prediction" Q values
                output = self.DQN.forward(state)
                y_pred = output.gather(1, action.long().view((self.batch, 1))).view(-1)
                
                # calculate "target" Q-values from Q-Learning update rule
                reward_indicator = (~reward.abs().bool()).int() #invert rewards {0 -> 1, {-1,1} -> 0}
                y_target = self.gamma * (reward + reward_indicator * self.target_network(next_state).max(dim=1).values)

                # forward + backward + optimize
                loss = self.DQN.criterion(y_pred, y_target)
                self.DQN.optimizer.zero_grad()
                loss.backward()
                self.DQN.optimizer.step()
                self.losses.append(loss.detach().numpy())

            # update target network
            if i % self.update_target == 0:
                self.target_network.load_state_dict(self.DQN.state_dict())


            # save results
            if winner == self.player:
                history.append(1)
            elif winner == agent.player:
                history.append(-1)
            else:
                history.append(0)

        return history
              

class DeepQNetwork(nn.Module):
    def __init__(self, alpha=10e-5, delta=1, hidden_neurons=128):
        super(DeepQNetwork, self).__init__()
        self.linear1 = nn.Linear(18, hidden_neurons)
        self.linear2 = nn.Linear(hidden_neurons, hidden_neurons)
        self.linear3 = nn.Linear(hidden_neurons, 9)

        self.optimizer = torch.optim.Adam(self.parameters(), lr=alpha)
        self.criterion = nn.HuberLoss(delta=delta)

    def forward(self, x):
        x = x.float()
        x = F.relu(self.linear1(x))
        x = F.relu(self.linear2(x))
        x = self.linear3(x)
        return x


class Memory:
    def __init__(self, size):
        self.size = size
        self.memory_used = 0
        self.reset_memory()

    def update(self, last_state, last_action, reward, grid):
        if isinstance(last_action, int) and isinstance(last_state, torch.Tensor):
            if self.memory_used < self.size:
                    self.memory_used += 1
            self.memory_s[self.position] = last_state
            self.memory_a[self.position] = last_action
            self.memory_r[self.position] = reward
            self.memory_s1[self.position] = grid_to_tensor(grid)
            self.position = (self.position + 1) % self.size

    def sample(self, batch):
        idx = torch.randperm(self.memory_used)[:batch].long()
        return self.memory_s[idx], self.memory_a[idx], self.memory_r[idx], self.memory_s1[idx]

    def reset_memory(self):
        self.memory_s = torch.empty((self.size, 18))
        self.memory_a = torch.empty((self.size))
        self.memory_r = torch.empty((self.size))
        self.memory_s1 = torch.empty((self.size, 18))
        self.position = 0

   
