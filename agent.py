from abc import abstractclassmethod
import numpy as np
from tic_env import TictactoeEnv, OptimalPlayer
from tqdm import tqdm
from collections import defaultdict
from utils import *
import torch
import torch.nn as nn
import torch.nn.functional as F
import random

class Agent:
    def __init__(self, player):
        self.player = player
        self.other_player = str(({'X', 'O'} - set(self.player)).pop())

    def change_player(self):
        self.player = str(({'X', 'O'} - set(self.player)).pop())

    def empty(self, grid):
        '''return all empty positions'''
        avail = []
        for i in range(9):
            pos = (int(i/3), i % 3)
            if grid[pos] == 0:
                avail.append(pos)
        return avail
 
    def randomMove(self, grid):
        """ Chose a random move from the available options. """
        avail = self.empty(grid)
        return avail[random.randint(0, len(avail)-1)]

    @abstractclassmethod
    def act(self):
        pass

class QPlayer(Agent):
    def __init__(self, epsilon=0.2, alpha=0.05, gamma=0.99, player='X', explore=False, e_min=0.1, e_max=0.8, n_star=20000):
        super().__init__(player)
        self.epsilon = epsilon
        self.alpha = alpha
        self.gamma = gamma
        self.Qvalues = defaultdict(lambda: defaultdict(int))
        self.last_action = 0
        self.last_state = 0
        self.player2value = {'X': 1, 'O': -1}
        self.counter = 0
        self.explore = explore
        self.e_min = e_min
        self.e_max = e_max
        self.n_star = n_star

    def act(self, grid):
        if self.explore:
            self.epsilon = self.decreasing_exploration(self.e_min, self.e_max, self.n_star, self.counter)
        # epsilon greedy policy w.r.t. Q-values
        if random.random() < self.epsilon:
            action = self.randomMove(grid)
            self.last_action = tuple_to_int(action)
        else:
            idx = np.argmax([self.Qvalues[str(grid)][tuple_to_int(pos)] for pos in self.empty(grid)])
            action = self.empty(grid)[idx]
            self.last_action = tuple_to_int(action)

        self.last_state = str(grid)  
        return action

    def updateQ(self, grid, reward):
        future_estimate = max([self.Qvalues[str(grid)][tuple_to_int(pos)] for pos in self.empty(grid)], default=0)
        self.Qvalues[self.last_state][self.last_action] += self.alpha * (reward + self.gamma * future_estimate - self.Qvalues[self.last_state][self.last_action]) 
            
    def decreasing_exploration(self, e_min, e_max, n_star, n):
        return max(e_min, e_max * (1 - n/n_star)) 

class DeepAgent(Agent):
    def __init__(self, epsilon=0.2, gamma=0.99, buffer=10000, batch=64, update_target=500):
        self.gamma = gamma
        self.buffer = buffer
        self.batch = batch
        self.epsilon = epsilon
        self.update_target = update_target
        self.counter = 0
        self.DQN = DeepQNetwork()
        self.last_state = None
        self.last_action = None
        self.last_reward = None
        self.memory_s = torch.empty((self.buffer, 18))
        self.memory_a = torch.empty((self.buffer))
        self.memory_r = torch.empty((self.buffer))
        self.memory_s1 = torch.empty((self.buffer, 18))
        self.memory_used = 0

    def reset_memory(self):
        self.memory_s = torch.empty((self.buffer, 18))
        self.memory_a = torch.empty((self.buffer))
        self.memory_r = torch.empty((self.buffer))
        self.memory_s1 = torch.empty((self.buffer, 18))
        self.memory_used = 0

    def update_memory(self, grid, reward):
        if self.memory_used < self.buffer:
            if isinstance(self.last_action, int) and isinstance(self.last_state, torch.Tensor):
                self.memory_s[self.memory_used] = self.last_state
                self.memory_a[self.memory_used] = self.last_action
                self.memory_r[self.memory_used] = reward
                self.memory_s1[self.memory_used] = grid_to_tensor(grid)
                self.memory_used += 1

    def sample_learning_data(self):
        idx = torch.randperm(self.memory_used)[:self.batch].long()
        output = self.DQN.forward(self.memory_s[idx])
        y_pred = output.gather(1, self.memory_a[idx].long().view((self.batch,1))).view(-1)
        y_target = self.memory_r[idx] + self.DQN.forward(self.memory_s1[idx]).max(dim=1).values
        return y_pred, y_target

    def act(self, grid):
        # epsilon greedy w.r.t. Qvalues generated by Deep Network
        if random.random() < self.epsilon:
            action = self.randomMove(grid)
            self.last_action = tuple_to_int(action)
        else:
            action = self.DQN(grid_to_tensor(grid)).argmax().item()
            self.last_action = action

        self.last_state = grid_to_tensor(grid)  
        return action

    def play_game(self, agent, env, i):
        grid, end, __  = env.observe()
        states = []
        if i % 2 == 0:
            self.player = 'X'
            agent.player = 'O'
        else:
            self.player = 'O'
            agent.player = 'X'
        while end == False:
            if env.current_player == self.player:
                move = self.act(grid) 
                grid, end, winner = env.step(move, print_grid=False)
            else:
                move = agent.act(grid)
                grid, end, winner = env.step(move, print_grid=False) 

                reward = env.reward(self.player)
                self.update_memory(grid.copy(),reward)
            states.append(grid.copy())
        return winner, states

    
    def learn(self, agent, N=20000):
        history = []
        env = TictactoeEnv()
        for i in tqdm(range(2, N+2)):
            env.reset()
            winner = self.play_game(agent, env, i) 

            # train phase
            if i % self.update_target == 0:
                y_pred, y_target = self.sample_learning_data()

                # zero the parameter gradients
                self.DQN.optimizer.zero_grad()

                # forward + backward + optimize
                loss = self.DQN.criterion(y_pred, y_target)
                loss.backward()
                self.DQN.optimizer.step()

            # save results
            if winner == self.player:
                history.append(1)
            elif winner == agent.player:
                history.append(-1)
            else:
                history.append(0)

        return history
              

class DeepQNetwork(nn.Module):
    def __init__(self):
        super(DeepQNetwork, self).__init__()
        self.linear1 = nn.Linear(18, 128)
        self.linear2 = nn.Linear(128, 128)
        self.linear3 = nn.Linear(128, 9)

        self.optimizer = torch.optim.Adam(self.parameters(), lr=10e-5)
        self.criterion = nn.HuberLoss(delta=1)

    def forward(self, x):
        x = x.float()
        x = F.relu(self.linear1(x))
        x = F.relu(self.linear2(x))
        x = self.linear3(x)
        return x